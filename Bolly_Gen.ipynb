{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bolly_Gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORdig7aeOoG/YQ8Z9uoiw8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygsa/BollyGen/blob/master/Bolly_Gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wEmAcBw3Dpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d0552f10-1f71-4de4-8ff3-c6ab589e9d3c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ZEiMi33Lt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data=pd.read_csv(\"/content/drive/My Drive/All_Lyrics/Hindi_Songs_DS.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jdotAba9nLI",
        "colab_type": "text"
      },
      "source": [
        "I uploaded the Songs dataset on Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlL32Hnb3cEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "d19b9e6e-c459-453b-9e2e-5754a142f16a"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class OneHotWayEncoding():\n",
        "\n",
        "    def dataimport(self):\n",
        "        path = '/content/drive/My Drive/All_Lyrics/Hindi_Songs_DS.csv'\n",
        "        self.df = pd.read_csv(path)\n",
        "        print(self.df.head(5))\n",
        "\n",
        "    def tocorpus(self):\n",
        "        self.text = self.df['Lyrics'].str.cat(sep='\\n').lower()\n",
        "        # Output the length of the Corpus\n",
        "        print('corpus length : ', len(self.text))\n",
        "\n",
        "        # Create a sorted list of the Characters\n",
        "        self.chars = sorted(list(set(self.text)))\n",
        "        print('Total Chars : ', len(self.chars))\n",
        "\n",
        "        # Reducing the length of Corpus\n",
        "        #self.text = self.text[:900999]\n",
        "        #print('Truncated Corpus Length : ', len(self.text))\n",
        "\n",
        "    def featurelabeldataset(self):\n",
        "        # Create a dictionary where given a character, you can look up the index and vice versa\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "        # cut the text in semi-redundant sequences of maxlen characters\n",
        "        self.maxlen = 40  # The window size\n",
        "        self.step = 3  # The steps between the windows\n",
        "        sentences = []\n",
        "        next_chars = []\n",
        "\n",
        "        # Step through the text via 3 characters at a time, taking a sequence of 40 bytes at a time.\n",
        "        # There will be lots ofo overlap\n",
        "        for i in range(0, len(self.text) - self.maxlen, self.step):\n",
        "            sentences.append(self.text[i: i + self.maxlen])  # range from current index i for max length characters\n",
        "            next_chars.append(self.text[i + self.maxlen])  # the next character after that\n",
        "\n",
        "        self.sentences = np.array(sentences)\n",
        "        self.next_chars = np.array(next_chars)\n",
        "        print('Number of sequences:', len(sentences))\n",
        "\n",
        "        data = {'Sentences': sentences, 'Next Char': next_chars}\n",
        "\n",
        "        # Create DataFrame\n",
        "        newdf = pd.DataFrame(data)\n",
        "\n",
        "        # Print the output.\n",
        "        print(newdf)\n",
        "\n",
        "    def getdata(self,sentences, next_chars):\n",
        "        X = np.zeros((len(sentences), self.maxlen, len(self.chars)), dtype=np.bool)\n",
        "        y = np.zeros((len(sentences), len(self.chars)), dtype=np.bool)\n",
        "        length = len(sentences)\n",
        "        index = 0\n",
        "        for i in range(len(sentences)):\n",
        "            sentence = sentences[i]\n",
        "            for t, char in enumerate(sentence):\n",
        "                X[i, t, self.char_indices[char]] = 1\n",
        "            y[i, self.char_indices[next_chars[i]]] = 1\n",
        "        return X, y\n",
        "\n",
        "    def buildmodel(self):\n",
        "        # build the model: a single LSTM\n",
        "        print('Build model...')\n",
        "        self.model = Sequential()\n",
        "        self.model.add(LSTM(128, input_shape=(self.maxlen, len(self.chars))))\n",
        "        self.model.add(Dense(len(self.chars)))\n",
        "        self.model.add(Activation('softmax'))\n",
        "        self.model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "        print(\"Compiling model complete...\")\n",
        "\n",
        "    def trainingmodel(self):\n",
        "        self.dataimport()\n",
        "        self.tocorpus()\n",
        "        self.featurelabeldataset()\n",
        "        self.buildmodel()\n",
        "        X, y = self.getdata(self.sentences, self.next_chars)\n",
        "        # The training\n",
        "        print('Training...')\n",
        "        # Use this if they all fit into memory\n",
        "        history = self.model.fit(X, y, batch_size=128, epochs=50, verbose=0)\n",
        "        # Save the model\n",
        "        self.model.save('onehot-modelweights.h5')\n",
        "\n",
        "    def sample(init,preds, temperature=1.0):\n",
        "        preds = np.asarray(preds).astype('float64')\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        probas = np.random.multinomial(1, preds, 1)\n",
        "        return np.argmax(probas)\n",
        "\n",
        "    def runmodel(self,sentence):\n",
        "        variance = 0.4\n",
        "        generated = ''\n",
        "        original = sentence\n",
        "        window = sentence\n",
        "        for i in range(400):\n",
        "            x = np.zeros((1, self.maxlen, len(self.chars)))\n",
        "            for t, char in enumerate(window):\n",
        "                x[0, t, self.char_indices[char]] = 1.\n",
        "\n",
        "            preds = self.model.predict(x, verbose=0)[0]\n",
        "            next_index = self.sample(preds, variance)\n",
        "            next_char = self.indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            window = window[1:] + next_char\n",
        "\n",
        "        print(original + generated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = OneHotWayEncoding()\n",
        "    start.trainingmodel()\n",
        "    start.runmodel(\"sapno mein khoya deewana sakshi kaha hai\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0                                             Lyrics\n",
            "0           0  \\n\\naah ko chaahiye ik umra asar honay tak\\nka...\n",
            "1           1  \\n\\n aaaa\\naeyy ghar ye tera sadaa na mera hai...\n",
            "2           2  \\n\\n haay kis but ki muhabbat mein giraftaar h...\n",
            "3           3  \\n\\n song idhar phir bhi aana udhar jaane waal...\n",
            "4           4  \\n\\n aana meri jan meri jan\\nsunday ke sunaday...\n",
            "corpus length :  10954826\n",
            "Total Chars :  28\n",
            "Number of sequences: 3651596\n",
            "                                          Sentences Next Char\n",
            "0        \\n\\naah ko chaahiye ik umra asar honay tak        \\n\n",
            "1         ah ko chaahiye ik umra asar honay tak\\nka         u\n",
            "2         ko chaahiye ik umra asar honay tak\\nkaun          j\n",
            "3         chaahiye ik umra asar honay tak\\nkaun jee         t\n",
            "4         ahiye ik umra asar honay tak\\nkaun jeeta          h\n",
            "...                                             ...       ...\n",
            "3651591   aan hoon\\naapano me bhi khud se main to a         n\n",
            "3651592    hoon\\naapano me bhi khud se main to anja         a\n",
            "3651593   on\\naapano me bhi khud se main to anjaan          h\n",
            "3651594    aapano me bhi khud se main to anjaan hoo         n\n",
            "3651595   ano me bhi khud se main to anjaan hoon \\n        \\n\n",
            "\n",
            "[3651596 rows x 2 columns]\n",
            "Build model...\n",
            "Compiling model complete...\n",
            "Training...\n",
            "sapno mein khoya deewana sakshi kaha hai\n",
            "mere saath hai yeh dil ka kar de\n",
            "pyaar ki khatal hai hai hai\n",
            "kya hai maine kahan hota hai\n",
            "\n",
            "tu hai to har hai meri\n",
            "meri baat hai tu\n",
            "mere dil kii dori meri hai teri\n",
            "tujhse meri khushi ki baat karti hai\n",
            "meri aankhon mein khoyi hai tu hi tu hai ya hai\n",
            "tera naam hai tera hai\n",
            "\n",
            "teri baat mein teri\n",
            "meri hai main kya karuun\n",
            "teri maanga ke dil kaa dard hai tujhe\n",
            "tune meri zindagi mein teri hai tu\n",
            "meri tu h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76KkI4RKYDtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start.runmodel(\"sapno mein khoya deewana sakshi kaha hai\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}